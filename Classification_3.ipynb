{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classification_3.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"collapsed":true,"id":"m3i_6d7eJKLl","colab_type":"text"},"cell_type":"markdown","source":["# Μια ολοκληρωμένη διαδικασία ταξινόμησης\n","\n","Το scikit-learn έχει καθιερώσει την ορολογία που περιγράφει με κομψό τρόπο μια ολοκληρωμένη διαδικασία Μachine Learning, στην περίπτωσή που εξετάζουμε μια διαδικασία ταξινόμησης.\n","\n","## Pipeline, Εκτιμητές και Μετασχηματιστές\n","\n","Μια διαδικασία ML ή workflow ή pipeline αποτελείται από μια σεiρα μετασχηματιστών πάνω στα χαρακτηριστικά των δεδομένων που τελειώνει σε έναν εκτιμητή.\n","\n","![](https://sv3nd.github.io/images/scikit-learn-pipeline-post-processing_sk_pipeline.png)\n","\n","Οι μετασχηματιστές χρησιμοποιούνται για να κανουν την προεπεξεργασία (μέσω μετασχηματισμού) των δεδομένων. Είδαμε στο προηγούμενο notebook πέντε μετασχηματιστές: την επιλογή χαρακτηριστικών VarianceThreshold, δύο μετασχηματιστές κανονικοποίησης (τον scaler και τον min_max_scaler), τον εξισορροπητή με τυχαία υπερδειγματοληψία RandomOverSampler και την εξαγωγή χαρακτηριστικών PCA. Αν ανατρέξετε στο προηγούμενο notebook θα δείτε ότι κάνουν fit και transform στο train set και transform στο test set.\n","\n","Οι μετασχηματιστές γενικα έχουν και αυτοί υπερ-παραμέτρους που επηρρεάζουν τη λειτουργία τους: ο VarianceThreshold είχε το κατώτερο κατώφλι διακύμανσης ο PCA τον αριθμό των κύριων συνιστωσών, ενώ ακόμα και οι scaler, min_max_scaler και RandomOverSampler έχουν αλλά δεν τις εξετάσαμε. Όπως έχουμε πει η επιλογή των υπερ-παραμέτρων (όπως το k του kNN) γίνεται μόνο εμπειρικά μέσω διασταυρούμενης επικύρωσης (cross-validation). Οι μετασχηματιστές και οι υπερπαράμετροι τους επιδρούν λοιπόν στη μορφή των δεδομένων.\n","\n","Στο τέλος του pipeline VarianceThreshold - scaler - RandomOverSampler - PCA βάλαμε τον εκτιμητή - ταξινομητή MultiLayerPerceptron. O MLP έχει και αυτός υπερ-παραμέτρους και μάλιστα έναν πολύ μεγάλο αριθμό (18 για την ακρίβεια): πλήθος και επίπεδα κρυμμένων νευρώνων, συνάρτηση ενεργοποίησης, βελτιστοποίησης κλπ. Στο προηγούμενο notebook χρησιμοποιήσαμε ένα MLP με σταθερές υπερπαραμέτρους, ωστόσο σε μια ολοκληρωμένη διαδικασία ML οι υπερ-παράμετροι (ή κάποιες από τις υπερπαραμέτρους) του ταξινομητή πρέπει και αυτές να βελτιστοποιηθούν με διαδικασία cross-validation. Οι ταξινομητές και οι υπερπαράμετροι τους δεν επιδρούν στη μορφή των δεδομένων όπως οι μετασχηματιστές αλλά έχουν προφανώς επίδραση στην απόδοση του μοντέλου.\n","\n","## Ορισμός (επιλογή) ενός εκπαιδευμένου μοντέλου εκτιμητή (ταξινομητή)\n","\n","Μια ολοκληρωμένη διαδικασία pipeline λόγω του ότι τελειώνει σε έναν εκτιμητή μπορεί να θεωρηθεί και συνολικά ως ένας εκτιμητής, με κανένα, με λίγους ή με περισσότερους μετασχηματιστές πριν από τον εκτιμητή. Στο προηγούμενο παράδειγμα είδαμε στην αρχή ένα μοντέλο με μόνο τον εκτιμητή (εφαρμογή του MLP απευθείας στο dataset) και ένα τελικό μοντέλο με τέσσερεις μετασχηματιστές πριν τον εκτιμητή. Ένα εκπαιδευμένο μοντέλο εκτιμητή (ταξινομητή) αποτελείται\n","- α) απο την αρχιτεκτονική του, δηλαδή τον συνδυασμό μετασχηματιστών και την επιλογή του τελικού εκτιμητή (το pipeline), και \n","- β) από τις (βέλτιστες) τιμές των υπερ-παραμέτρων όλων των προηγουμένων που προκύπτουν από το cross-validation. \n","\n","<img src=\"https://github.com/rasbt/pattern_classification/raw/master/Images/supervised_learning_flowchart.png\" width=\"65%\">\n","\n","Το τελικό βελτιστοποιημένο μοντέλο αποτιμάται στα δεδομένα test και χρησιμοποιείται για να κάνει προβλέψεις σε νέα δεδομένα.\n","\n","ΠΡΟΣΟΧΗ: εκτός από τη διαχείριση τιμών που απουσιάζουν με Imputer, όλοι οι υπόλοιποι μετασχηματιστές βρίσκονται εντός του σχήματος crossvalidation. \n","\n","Η μετατροπή κατηγορικών μεταβλητών γίνεται μετά τη διαχείριση τιμών που απουσιάζουν και πριν το crossvalidation (εκτός δλδ)."]},{"metadata":{"id":"lIoeOdaWJKLn","colab_type":"text"},"cell_type":"markdown","source":["## Βελτιστοποίηση ύπερ-παραμέτρων (Hyperparameter optimization)\n","\n","Είδαμε ότι τόσο οι μετασχηματιστές όσο και οι εκτιμητές έχουν υπερ-παραμέτρους που πρέπει να βελτιστοποιηθούν με cross-validation. Εφόσον τόσο οι μετασχηματιστές όσο και ο εκτιμητής αποτελούν μέρος ενός ενιαίου pipeline, για να βρούμε τις βέλτιστες τιμές όλων των υπερ-παραμέτρων μέσω cross-validation θα πρέπει\n","- α) για κάθε fold του cross-validation, να υπολογίσουμε την απόδοση όλων των πιθανών συνδυασμών υπερ-παραμέτρων μετασχηματιστών και εκτιμητή και \n","- β) να επιλέξουμε το συνδυασμό υπερ-παραμέτρων που έχει τον καλύτερο μέσο όρο με βάση κάποια μετρική σε όλα τα folds.\n","\n","\n","### Αναζήτηση πλέγματος (grid search)\n","\n","Η απόδοση όλων των πιθανών συνδυασμών υπερ-παραμέτρων μετασχηματιστών και εκτιμητή γίνεται με αναζήτηση πλέγματος (grid search). Ορίζουμε για κάθε παράμετρο ένα πεδίο ορισμού, συνήθως με ελάχιστο, μέγιστο και κάποιο βήμα και φτιάχνουμε ένα πλέγμα με όλους τους πιθανούς συνδυασμούς τιμών των παραμέτρων. Για παράδειγμα:\n","\n","για κύριες συνιστώσες PCA (transformer) από 5 μέχρι 15 με βήμα 5 και για έναν kNN (estimator) με k από 1 μέχρι 5 με βήμα 2 παίρνουμε το ακόλουθο grid:\n","\n","|     |       |        |        |\n","|-----|-------|--------|--------|\n","|     | PC=5  | PC=10  | PC=15  |\n","| k=1 | (1,5) | (1,10) | (1,15) |\n","| k=3 | (3,5) | (3,10) | (3,15) |\n","| k=5 | (5,5) | (5,10) | (5,15) |\n","\n","Για κάθε τιμή υπερπαραμέτρων του grid θα πρέπει να υπολογιστεί ο μέσος όρος του εκτιμητή σε όλα τα folds του cross-validation με βάση το metric (πχ F1) και να επιλεχθεί ο καλύτερος συνδυασμός παραμέτρων. Η συγκεκριμένη στρατηγική αναζήτησης των βέλτιστων υπερπαραμέτρων είναι η εξαντλητική αναζήτηση πλέγματος (exhaustive grid search) και είναι προφανώς πολύ ακριβή υπολογιστικά. Υπάρχουν διάφορες τεχνικές για να περιορίζεται η πολυπλοκότητα του grid search, αλλά δεν το αποφεύγουμε γενικά, γιατί οι υπερπαράμετροι είναι ορίσματα των εκτιμητών και δεν μαθαίνονται από την fit.\n","\n","Συνοψίζοντας, η βελτιστοποίηση των υπερπαραμέτρων απαιτεί \n","\n","- έναν εκτιμητή (έναν ταξινομητή)\n","- τον πεδίο ορισμού των υπερπαραμέτρων\n","- ένα τρόπο αναζήτησης των πιθανών συνδυασμών τιμών τους πχ grid search\n","- ένα σχήμα cross-validation πχ 5-fold\n","- μια μετρική απόδοσης (ή score) πχ F1-macro\n","\n","Το scikit-learn μας απλοποιεί σε πολύ μεγάλο βαθμό την κατασκευή pipelines και τη βελτιστοποίηση των υπερπαραμέτρων. Θα το δούμε με ένα παράδειγμα."]},{"metadata":{"id":"E1wVUP0oJKLo","colab_type":"text"},"cell_type":"markdown","source":["Θα βελτιστοποιήσουμε με cross-validation και grid search ένα pipeline με προ-επεξεργασία των δεδομένων από transformers και estimator τον kNN."]},{"metadata":{"id":"QnFEJHRAJKLp","colab_type":"text"},"cell_type":"markdown","source":["# MNIST handwritten digits dataset\n","Ενημερώνουμε τις βιβλιοθήκες μας. Επίσης θα αγνοήσουμε κάποια warnings"]},{"metadata":{"id":"60DxH4hqv5g7","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install -U scikit-learn\n","!pip install -U numpy\n","!pip install -U pandas\n","import warnings \n","warnings.filterwarnings('ignore')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZitmYTW15d9Z","colab_type":"text"},"cell_type":"markdown","source":["\n","Ένας ακόμη τρόπος να εισάγουμε datasets είναι από το [mldata.org](http://mldata.org/), ένα ανοικτό αποθετήριο datasets. Θα εισάγουμε το πολύ γνωστό και απο το Deep Learning dataset [MNIST](http://yann.lecun.com/exdb/mnist/). "]},{"metadata":{"scrolled":true,"id":"YmrhYkpAJKLr","colab_type":"code","colab":{}},"cell_type":"code","source":["# import numpy as np\n","# Αυτή τη στιγμή το mldata.org είναι πεσμένο\n","# from sklearn.datasets import fetch_mldata\n","# mnist = fetch_mldata('MNIST original')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TibbJWFbfd0-","colab_type":"text"},"cell_type":"markdown","source":["Θα το εισάγουμε από έναν άλλο server όπου υπάρχει ένα αντίγραφο. Το dataset είναι επίσης διαθέσιμο στο [Kaggle](https://www.kaggle.com/gustavoatt/mnist-original)."]},{"metadata":{"id":"gZYdK0xHfXwo","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","mnist_train = pd.read_csv(\"https://www.python-course.eu/data/mnist/mnist_train.csv\", header=None).values\n","mnist_test = pd.read_csv(\"https://www.python-course.eu/data/mnist/mnist_test.csv\", header=None).values\n","mnist = np.concatenate((mnist_train, mnist_test), axis=0) # ενώνουμε train και test\n","# εναλλακτικά αν έχει μεγάλο φόρτο το python-course\n","# mnist = pd.read_csv(\"https://dodo.islab.ntua.gr/mnist.csv\", header=None).values"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4NATnixruUjy","colab_type":"code","colab":{}},"cell_type":"code","source":["features = mnist[:, 1:]\n","targets = mnist[:, :1] # τα labels είναι στην πρώτη κολώνα"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oyBI-FW5JKLw","colab_type":"text"},"cell_type":"markdown","source":["To MNIST περιλαμβάνει 70000 δείγματα χειρόγραφων ψηφίων μεγέθους 28x28 pixels, με ετικέτες από το 0 ως το 9. Τα 28x28 pixels κάθε δείγματος αντιστοιχούν σε 768 χαρακτηριστικά με τιμές του γκρι από 0 (μάυρο) εώς 256 (λευκό)"]},{"metadata":{"id":"wqaNnxHVJKLx","colab_type":"code","colab":{}},"cell_type":"code","source":["print(features.shape)\n","print(targets.shape)\n","print(np.unique(targets)) #τυπώνουμε τις μοναδικές ετικέτες των labels"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b-KkvXLPJKL4","colab_type":"text"},"cell_type":"markdown","source":["Θα κάνουμε οπτικοποίηση ενός δείγματος:"]},{"metadata":{"id":"JucaKnxDJKL6","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","pixels = features[50000]\n","pixels = pixels.reshape((28, 28))\n","plt.imshow(pixels, cmap='gray')\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xiOZZIntJKL_","colab_type":"text"},"cell_type":"markdown","source":["Επειδή το dataset είναι μεγάλο (ειδικά για τον kNN) και για το παράδειγμά μας θέλουμε να δουλέψουμε με λιγότερα δείγματα. Το ανακατεύουμε και παίρνουμε ένα μικρό αριθμό samples"]},{"metadata":{"scrolled":true,"id":"fGJyXQPqJKMA","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.utils import shuffle\n","sdata, starget = shuffle(features, targets, random_state=341976)\n","samples = 1000\n","data = sdata[0:samples-1,:]\n","target = starget[0:samples-1]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QTHa32MzJKME","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=20176)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OP_af-MiJKMJ","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn import neighbors\n","from sklearn.metrics import classification_report\n","clf = neighbors.KNeighborsClassifier()\n","clf.fit(X_train,y_train)\n","preds = clf.predict(X_test)\n","print(classification_report(y_test, preds))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4OEeXtfRHaCO","colab_type":"text"},"cell_type":"markdown","source":["# Pipelines"]},{"metadata":{"id":"PSLRuvjxJKMN","colab_type":"text"},"cell_type":"markdown","source":["Για την κατασκευή του μοντέλου θα βασιστούμε στην κλάση Pipeline. Επειδή οι κλάσεις εξισορρόπησης του imblearn όπως η [RandomOverSampler](http://contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.over_sampling.RandomOverSampler.html) τυπικά δεν έχουν μέθοδο transform (έχουν fit_sample) η built-in Pipeline του scikit (from sklearn.pipeline import Pipeline) δεν τις δέχεται ως transformers. Θα φέρουμε την Pipeline από το imblearn (που έχει transform για τους samplers)."]},{"metadata":{"id":"cPkbvAyTwjdN","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install --upgrade imbalanced-learn"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JrLrdMceJKMO","colab_type":"code","colab":{}},"cell_type":"code","source":["#from imblearn.pipeline import Pipeline\n","from imblearn.pipeline import Pipeline\n","\n","# φέρνουμε τις γνωστές μας κλάσεις για preprocessing\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.preprocessing import StandardScaler # φέρνουμε τον StandarScaler ως transformer που έχει .transform kai ΄όχι ως scale()\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.decomposition import PCA\n","\n","# αρχικοποιούμε τους εκτιμητές (μετασχηματιστές και ταξινομητή) χωρείς παραμέτρους\n","selector = VarianceThreshold()\n","scaler = StandardScaler()\n","ros = RandomOverSampler()\n","pca = PCA()\n","clf = neighbors.KNeighborsClassifier(n_jobs=-1) # η παράμετρος n_jobs = 1 χρησιμοποιεί όλους τους πυρήνες του υπολογιστή\n","pipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('sampler', ros), ('pca', pca), ('kNN', clf)])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ypk9FqwaJKMT","colab_type":"text"},"cell_type":"markdown","source":["Το pipeline συμπεριφέρεται ως ένας ενιαίος estimator. Μπορούμε να εφαρμόσουμε fit και predict."]},{"metadata":{"id":"TahYufqoJKMU","colab_type":"code","colab":{}},"cell_type":"code","source":["pipe.fit(X_train,y_train)\n","preds = pipe.predict(X_test)\n","print(classification_report(y_test, preds))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oGI4K_pXHhpd","colab_type":"text"},"cell_type":"markdown","source":["# GridsearchCV"]},{"metadata":{"id":"LkBQboyeJKMc","colab_type":"text"},"cell_type":"markdown","source":["Στη συνέχεια θα χρησιμποιήσουμε την [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) για να βελτιστοποιήσουμε τις υπερπαραμέτρους μας. Η GridSearchCV κάνει μαζί cross-validation και grid search. Αρχικά μελετάμε το variance των μετβλητών για τη variance threshold:"]},{"metadata":{"id":"8llFfWkU0ENO","colab_type":"code","colab":{}},"cell_type":"code","source":["train_variance = X_train.var(axis=0)\n","print(train_variance)\n","print(np.max(train_variance))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sWsZXzCI0BfT","colab_type":"text"},"cell_type":"markdown","source":[" Την εισάγουμε και θέτουμε τις τιμές ορισμού των υπερπαραμέτρων:"]},{"metadata":{"id":"acHBk9N6JKMc","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","vthreshold = [0, 4000, 8000, 12000] #προσαρμόζουμε τις τιμές μας στο variance που παρατηρήσαμε\n","n_components = [10, 20, 30, 40, 50, 60]\n","k = [1, 6, 11, 21, 31, 41] # η υπερπαράμετρος του ταξινομητή"],"execution_count":0,"outputs":[]},{"metadata":{"id":"s-1JhcWBJKMj","colab_type":"text"},"cell_type":"markdown","source":["Επειδή ο χώρος αναζήτησης των βέλτιστων υπερπαραμέτρων αρχίζει να μεγαλώνει, ξαναορίζουμε την pipeline με την παράμετρο 'memory': για κάθε fold του crossvalidation και για καθε συνδυασμό υπερπαραμέτρων μετασχηματιστών, τα δεδομένα χρειάζεται να μετασχηματιστούν μία φορά και όχι για κάθε νέα τιμή υπερπαραμέτρων του εκτιμητή. \n","\n","Είναι πιθανό στο fit να σας εμφανιστούν κάποια warnings με τη χρήση του memory. Ξαναστρέξτε το block του κώδικα."]},{"metadata":{"id":"pjQapwjLJKMl","colab_type":"code","colab":{}},"cell_type":"code","source":["pipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('sampler', ros), ('pca', pca), ('kNN', clf)], memory = 'tmp')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AXyzuq4zJKMp","colab_type":"text"},"cell_type":"markdown","source":["Μπορούμε να θέτουμε τιμές στις υπερπαραμέτρους των pipelines χρησιμοποιώντας τα ονόματα των estimators, \"\\_\\_\", το όνομα της υπερπαραμέτρου, \"=\" και τις τιμές που της δίνουμε στο grid search. Επίσης μπορούμε να θέσουμε τη μετρική της απόδοσης με την παράμετρο \"scoring\". Με την παράμετρο \"cv\" ορίζουμε τον αριθμό των folds. Για βελτιστοποίηση, μπορούμε να θέσουμε την παράμετρο n_jobs=-1 ώστε να χρησιμοποιούνται όλοι οι πυρήνες του υπολογιστή (το default είναι 1)."]},{"metadata":{"id":"N9aHef9cJKMs","colab_type":"code","colab":{}},"cell_type":"code","source":[" estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold,pca__n_components=n_components, kNN__n_neighbors=k), cv=5, scoring='f1_macro', n_jobs=-1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"flUq4SjOJKMw","colab_type":"text"},"cell_type":"markdown","source":["Το GridSearchCV είναι επίσης ένας estimator με fit και predict. Ανάλογα το search space η εκτέλεση του  GridSearchCV μπορεί να πάρει αρκετό χρόνο"]},{"metadata":{"id":"BIE3BBGQJKMz","colab_type":"code","colab":{}},"cell_type":"code","source":["import time\n","start_time = time.time()\n","estimator.fit(X_train, y_train)\n","preds = estimator.predict(X_test)\n","print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n","print(classification_report(y_test, preds))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pZHju2CcJKM6","colab_type":"text"},"cell_type":"markdown","source":["Tυπώνουμε τον καλύτερο estimator και τον καλύτερο συνδυασμό υπερπαραμέτρων:"]},{"metadata":{"id":"w3LAEHBRJKM7","colab_type":"code","colab":{}},"cell_type":"code","source":["print(estimator.best_estimator_)\n","print(estimator.best_params_)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XYH-AL6GJKNA","colab_type":"text"},"cell_type":"markdown","source":["Η στοχαστικότητα στη διαδικασία της ταξινόμησης οφείλεται στα διαφορετικό διαχωρισμό σε folds σε κάθε run αλλά στον RandomOverSampler που επιλέγει τυχαία δείγματα κατά τη δειγματοληψία."]},{"metadata":{"id":"yBK0eKkKJKNN","colab_type":"text"},"cell_type":"markdown","source":["## Επιλογή αρχιτεκτονικής μοντέλου pipeline\n","\n","Προσοχή, η βέλτιστη αρχιτεκτονική δεν είναι δεδομένη αλλά εξαρτάται από το dataset. Δοκιμαστε στο ίδιο grid, χωρίς scaler και sampler"]},{"metadata":{"id":"FhG5uSl6JKNO","colab_type":"code","colab":{}},"cell_type":"code","source":["pipe = Pipeline(steps=[('selector', selector),('pca', pca), ('kNN', clf)], memory = 'tmp')\n","estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold, pca__n_components=n_components, kNN__n_neighbors=k), cv=3, scoring='f1_macro', n_jobs=-1)\n","# ο estimator με βελτιστοποιημένες υπερπαραμέτρους είναι έτοιμος να κάνει prediction.\n","# Ωστόσο για να μην πάνε χαμένα δεδομένα (ο΄ύτε ένα fold), τον κάνουμε fit σε όλα τα δεδομένα train.\n","estimator.fit(X_train, y_train)\n","preds = estimator.predict(X_test)\n","print(classification_report(y_test, preds))\n","print(estimator.best_estimator_)\n","print(estimator.best_params_)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H9HF7b5_2aDZ","colab_type":"code","colab":{}},"cell_type":"code","source":["estimator.get_params()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d3D_2jlnJKNS","colab_type":"text"},"cell_type":"markdown","source":["Και φυσικά θα μπορούσαμε να έχουμε διαφορετικά αποτελέσματα εφαρμόζοντας min max scaler αντι standard scaler, undersampling αντί oversampling κοκ. Προφανώς η πιο σημαντική απόφαση στην αρχιτεκτονική του ταξινομητή είναι η επιλογή του τελικού estimator,  αν πχ βάλουμε MLP ή SVM αντί kNN, και βέβαια η βελτιστοποίηση των υπερπαραμέτρων τους."]},{"metadata":{"id":"PX3ZkL7OJKNV","colab_type":"text"},"cell_type":"markdown","source":["## Progressive grid search\n","\n","Στο πεδίο ορισμού των παραμέτρων, ξεκινάμε με μεγάλα διαστήματα και σχετικά λίγα βήματα. Αν διαπιστώσουμε ότι υπαρχει μια περιοχή τιμών κάποιας παραμέτρου που δίνει καλη απόδοση μπορούμε να μικρύνουμε το διάστημα του grid search γύρω της και να βάλουμε περισσότερα βήματα."]},{"metadata":{"scrolled":true,"id":"pu8PlyLzJKNW","colab_type":"code","colab":{}},"cell_type":"code","source":["vthreshold = [0]\n","n_components = [39, 40, 41]\n","k = [1, 3]\n","estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold, pca__n_components=n_components, kNN__n_neighbors=k), scoring='f1_macro', n_jobs=-1)\n","estimator.fit(X_train, y_train)\n","preds = estimator.predict(X_test)\n","print(classification_report(y_test, preds))\n","print(estimator.best_estimator_)\n","print(estimator.best_params_)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KvSjUCRYJKNd","colab_type":"text"},"cell_type":"markdown","source":["Το περισσότερο fine grained grid search, αν δώσει καλύτερες τιμές θα έχει βελτιστοποιήσει τον εκτιμητή, αν όχι, τουλάχιστον θα επιβεβαιώσει ότι είμαστε σε ένα καλό τοπικό μέγιστο της συνάρτησης αξιολόγησης. Θα βρείτε και άλλες συμβουλές για το grid search στο [FAQ](https://docs.google.com/document/d/1jL4gRag_LHbVCYIt5XVJ53iJPb6RZWi02rT5mPXiqEU/edit?usp=sharing) της πρώτης άσκησης.\n","\n","---\n","\n"]}]}